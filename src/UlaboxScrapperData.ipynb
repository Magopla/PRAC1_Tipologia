{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1adb2e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document 'productsUrls.json' is available.\n",
      "\n",
      "Scraped url products loaded succesfuly.\n",
      "\n",
      "Enllaç complet al producte:  https://www.ulabox.com/ca/producte/aceite-de-oliva-suave-borges-2l/28350?ula_src=front_category_show&ula_mdm=product_list\n",
      "Added product\n",
      "\n",
      "\n",
      "Enllaç complet al producte:  https://www.ulabox.com/ca/producte/aceite-virgen-extra-priordei-500ml/131244?ula_src=front_category_show&ula_mdm=product_list\n",
      "Added product\n",
      "\n",
      "\n",
      "Enllaç complet al producte:  https://www.ulabox.com/ca/producte/aceite-virgen-extra-carbonell-1-l/7747?ula_src=front_category_show&ula_mdm=product_list\n",
      "Added product\n",
      "\n",
      "\n",
      "Enllaç complet al producte:  https://www.ulabox.com/ca/producte/aceite-de-oliva-virgen-extra-spar-1l/83625?ula_src=front_category_show&ula_mdm=product_list\n",
      "Added product\n",
      "\n",
      "\n",
      "Enllaç complet al producte:  https://www.ulabox.com/ca/producte/oli-verge-extra-borges-1l/45908?ula_src=front_category_show&ula_mdm=product_list\n",
      "Added product\n",
      "\n",
      "\n",
      "Enllaç complet al producte:  https://www.ulabox.com/ca/producte/aceite-de-oliva-suave-0-4-spar-5l/83629?ula_src=front_category_show&ula_mdm=product_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "def scraper_web(filename = \"\"):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            ulabox = json.load(f)             \n",
    "    except:\n",
    "        print(\"The document '\" + filename + \"' is not available.\\n\")\n",
    "        print(\"The scraper will retrieve all the products url.\\n\")\n",
    "        # call to init scraper of url's\n",
    "        # get the dict/json urls\n",
    "    else:\n",
    "        print(\"The document '\" + filename + \"' is available.\\n\")\n",
    "        print(\"Scraped url products loaded succesfuly.\\n\")\n",
    "    \n",
    "    # send the urls to the parser\n",
    "    return ulabox\n",
    "\n",
    "def dataframe_to_csv(data):\n",
    "    \n",
    "    data.to_csv('productes_alimentacio_ulabox.csv', index = False , encoding='utf-8-sig')\n",
    "    print(\"Dataset created successfuly\\n\")\n",
    "    \n",
    "    return  \n",
    "\n",
    "def scraper_get_all_products(dictionary_url):\n",
    "    '''\n",
    "    The scraper recieves the scraped url's, we parse the document and retrieve the information to finally create the dataset\n",
    "    '''\n",
    "    # Empty dataframe to store the scraped information of each products\n",
    "    df_products = pd.DataFrame(columns=['Id', 'Categoria', 'Subcategoria','Enllaç',\n",
    "                                        'Nom Producte','Preu','PreuBase','Ingredients',\n",
    "                                        'Valor Energètic Kj','Valor Energetic KC', 'Grases', 'Hidrats', 'Sucre', 'Proteines', 'Sal',\n",
    "                                        'Fabricant'])\n",
    "    \n",
    "    # Loop for retrieving and sraping each product\n",
    "    id = 0\n",
    "    for category in dictionary_url:\n",
    "        for subcategory in dictionary_url[category]:\n",
    "            for productsUrl in dictionary_url[category][subcategory]:\n",
    "                \n",
    "                # getting the url of the product\n",
    "                print(\"Enllaç complet al producte: \", baseUrl + productsUrl)\n",
    "                product_link = baseUrl + productsUrl\n",
    "                \n",
    "                # getting the soup and the product information\n",
    "                soup = scraper_get_soup(product_link)\n",
    "                product_info = scraper_get_products(soup, id, category, subcategory)\n",
    "                \n",
    "                # Adding the product to the dataframe\n",
    "                df_products = df_products.append(product_info, ignore_index = True)\n",
    "                print(\"Added product\\n\\n\")\n",
    "                \n",
    "                # Product control for ID's and time.sleep\n",
    "                id = id+1\n",
    "                #time.sleep()\n",
    "    \n",
    "                \n",
    "    # Saving the results of all products\n",
    "    #dataframe_to_csv(df_products)\n",
    "    df_products.to_csv('productes_alimentacio_ulabox.csv', index = False , encoding='utf-8-sig')\n",
    "    print(\"Dataset created successfuly\\n\")\n",
    "    return\n",
    "\n",
    "        \n",
    "\n",
    "def scraper_get_soup(url):\n",
    "    \n",
    "    UserAgent =  ({'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "    web = requests.get(url, headers = UserAgent)\n",
    "    soup = BeautifulSoup(web.content, 'html.parser')\n",
    "    \n",
    "    return soup  \n",
    "    \n",
    "def get_new_product(id, category, subcategory):\n",
    "    \n",
    "    product = {\n",
    "        'Id' : id,\n",
    "        'Categoria': category,\n",
    "        'Subcategoria': subcategory,\n",
    "        'Enllaç': \"Not Available\",\n",
    "        'Nom Producte': \"Not Available\",\n",
    "        'Preu': \"Not Available\",\n",
    "        'PreuBase': \"Not Available\",\n",
    "        'Ingredients': \"Not Available\",\n",
    "        'Valor Energètic Kj': \"Not Available\",\n",
    "        'Valor Energètic KC': \"Not Available\",\n",
    "        'Grases': \"Not Available\",\n",
    "        'Hidrats': \"Not Available\",\n",
    "        'Sucre': \"Not Available\",\n",
    "        'Proteines': \"Not Available\",\n",
    "        'Sal' : \"Not Available\",\n",
    "        'Fabricant' : \"Not Available\"\n",
    "    }\n",
    "    \n",
    "    return product\n",
    "\n",
    "def scraper_get_product_title(soup, product):\n",
    "    try:\n",
    "        name = soup.find(\"h1\").get_text()\n",
    "        product.update({'Nom Producte' : name})\n",
    "    except:\n",
    "        print(\"Product name not found\\n\")\n",
    "    \n",
    "    return product    \n",
    "    \n",
    "def scraper_get_product_link(soup, product):\n",
    "    try:\n",
    "        link_soup = soup.find('link', {'href': True, 'rel': 'canonical'})\n",
    "        link = link_soup['href']\n",
    "        product.update({'Enllaç' : link})\n",
    "    except:\n",
    "        print(\"Product link not found\\n\")\n",
    "    \n",
    "    return product\n",
    "\n",
    "def scraper_get_product_price(soup, product):\n",
    "    try:\n",
    "        price_soup = soup.find(\"meta\", { 'itemprop' : 'price'})\n",
    "        price = price_soup['content']\n",
    "        product.update({'Preu' : price})\n",
    "    except:\n",
    "        print(\"Product price not found\\n\")\n",
    "    \n",
    "    return product\n",
    "\n",
    "def scraper_get_product_base_price(soup, product):\n",
    "    try:\n",
    "        \n",
    "        classFilter = [ \"jss470\" ]\n",
    "        \n",
    "        subpreu = soup.find_all(\"p\")\n",
    "        basePrice = \"\"\n",
    "    \n",
    "        for preustotals in subpreu:\n",
    "            if preustotals['class'][1] in classFilter:\n",
    "                if basePrice != \"\":\n",
    "                    basePrice = basePrice + \" \" + preustotals.text\n",
    "                else:\n",
    "                    basePrice = preustotals.text\n",
    "        \n",
    "        product.update({'PreuBase' : basePrice})\n",
    "        \n",
    "    except:\n",
    "        print(\"Product base price not found\\n\")\n",
    "    \n",
    "    return product\n",
    "\n",
    "def scraper_get_product_nutritional_table(soup, product):\n",
    "    '''\n",
    "    The nutritional table may be found in ¿different layouts?:\n",
    "    - Catalan titles\n",
    "    - Spanish titles\n",
    "    ¿english titles?\n",
    "    '''\n",
    "    try:\n",
    "        \n",
    "        titols = soup.find_all(\"h6\")\n",
    "        apartatsText = [\n",
    "            \"Description\",\n",
    "            \"Ingredients\",\n",
    "            \"Usage and preservation\",\n",
    "            \"Additional information\"\n",
    "        ]\n",
    "        # Get divs for the table content\n",
    "        apartatsTaules = [\n",
    "            \"Nutrients\",\n",
    "            \"Measures\"\n",
    "        ]\n",
    "        \n",
    "        divs = {}\n",
    "        results = {}\n",
    "        valors_nutricionals = []\n",
    "        for titol in titols:\n",
    "            if(titol.text in apartatsText):\n",
    "                divs[titol.text] = titol.find_next('div')\n",
    "                results[titol.text] = divs[titol.text].text\n",
    "                \n",
    "            if(titol.text in apartatsTaules):\n",
    "                for sibling in titol.find_next_siblings():\n",
    "                # Eliminem el text que no ens interesa\n",
    "                # From: 'Energetic valueAprox.3700 KJ' to -> 3700\n",
    "                    valor_nutricional_numeric = sibling.text\n",
    "                    valors_nutricionals.append(re.search(r'\\d+', valor_nutricional_numeric).group())\n",
    "  \n",
    "        product.update({'Valor Energètic Kj' : valors_nutricionals[0]}) \n",
    "        product.update({'Valor Energètic KC' : valors_nutricionals[1]})\n",
    "        product.update({'Grases' : valors_nutricionals[2]})\n",
    "        product.update({'Hidrats' : valors_nutricionals[3]})\n",
    "        product.update({'Sucre' : valors_nutricionals[4]})\n",
    "        product.update({'Proteines' : valors_nutricionals[5]})\n",
    "        product.update({'Sal' : valors_nutricionals[6]})\n",
    "        \n",
    "    except:\n",
    "        print(\"Product nutritional table not found\\n\")\n",
    "    \n",
    "    return product\n",
    "\n",
    "def scraper_get_product_factory(soup, product):\n",
    "    try:\n",
    "        titols = soup.find_all(\"h6\")\n",
    "        apartatsText = [\n",
    "            \"Additional information\"\n",
    "        ]\n",
    "        \n",
    "        divs = {}\n",
    "        results = {}\n",
    "        valors_nutricionals = []\n",
    "        for titol in titols:\n",
    "            if(titol.text in apartatsText):\n",
    "                divs[titol.text] = titol.find_next('div')\n",
    "                results[titol.text] = divs[titol.text].text\n",
    "                factory = re.split(\": \", results[titol.text])[1]\n",
    "        \n",
    "        product.update({'Fabricant' : factory})\n",
    "            \n",
    "    except:\n",
    "        print(\"Product factory name not found\\n\")\n",
    "    \n",
    "    return product\n",
    "\n",
    "def scraper_get_products_ingredients(soup, product):\n",
    "    try:\n",
    "        titols = soup.find_all(\"h6\")\n",
    "        apartatsText = [\n",
    "            \"Ingredients\"\n",
    "        ]\n",
    "        \n",
    "        divs = {}\n",
    "        results = {}\n",
    "        valors_nutricionals = []\n",
    "        for titol in titols:\n",
    "            if(titol.text in apartatsText):\n",
    "                divs[titol.text] = titol.find_next('div')\n",
    "                results[titol.text] = divs[titol.text].text\n",
    "                ingredients = results[titol.text]\n",
    "        \n",
    "        product.update({'Ingredients' : ingredients})\n",
    "        \n",
    "    except:\n",
    "        print(\"Product ingredients not found\\n\")\n",
    "    \n",
    "    return product\n",
    "\n",
    "\n",
    "def load_requests(source_url):\n",
    "    print(source_url)\n",
    "    r = requests.get(source_url, stream = True)\n",
    "    if r.status_code == 200:\n",
    "        aSplit = source_url.split('/')\n",
    "        ruta = \"../imgs/\"+aSplit[len(aSplit)-1]\n",
    "        print(ruta)\n",
    "        try:\n",
    "            with open(ruta, 'wb') as f:\n",
    "                r.raw.decode_content = True\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "\n",
    "def scraper_get_product_image(soup, id):\n",
    "    try:        \n",
    "\n",
    "        image_html = soup.find(\"img\", { 'itemprop' : 'image'})\n",
    "        image_url = image_html['src']\n",
    "\n",
    "        r = requests.get(image_url, stream = True)\n",
    "        if r.status_code == 200:\n",
    "            aSplit = image_url.split('.')\n",
    "            ruta = \"../imgs/\"+str(id)+aSplit[-1]\n",
    "            with open(ruta, 'wb') as f:\n",
    "                r.raw.decode_content = True\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "            return ruta\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(\"Product image not found\\n\")\n",
    "    return\n",
    "\n",
    "def scraper_get_products(soup, id, category, subcategory):\n",
    "    \n",
    "    # Create an empty product\n",
    "    product = get_new_product(id, category, subcategory)\n",
    "    \n",
    "    # Retrieve information and storage in the product\n",
    "    #product = scraper_get_product_title(soup, product)\n",
    "    #product = scraper_get_product_link(soup, product)\n",
    "    #product = scraper_get_product_price(soup, product)\n",
    "    #product = scraper_get_product_base_price(soup, product)\n",
    "    #product = scraper_get_product_nutritional_table(soup, product)\n",
    "    #product = scraper_get_product_factory(soup, product)\n",
    "    #product = scraper_get_products_ingredients(soup, product)\n",
    "    \n",
    "    scraper_get_product_image(soup, id)\n",
    "    return product\n",
    "    \n",
    "    \n",
    "dictionary_url = scraper_web(\"productsUrls.json\")\n",
    "baseUrl = \"https://www.ulabox.com\"\n",
    "scraper_get_all_products(dictionary_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d747564f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Usuari\\\\Documents\\\\GitHub\\\\PRAC1_Tipologia\\\\src'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
